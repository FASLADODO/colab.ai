{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crowdcounting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM073PVhPXnTIx0DYGTZxtE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muasifk/colab.ai/blob/master/crowdcounting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfPCueqos1_p",
        "colab_type": "text"
      },
      "source": [
        "#**Crowd Counting**\n",
        "\n",
        "##**1. Introduction**\n",
        "\n",
        "Crowd Counting is a technique to **count** or **estimate** the number of people in an image or video.\n",
        "\n",
        "**Applications**\n",
        "\n",
        "\n",
        "1.   How many people attended a sporting event\n",
        "2.   How many people attended an inauguration or march (political rallies)\n",
        "3.   How many vehicles are on the road.\n",
        "\n",
        "> Crowd can be categorized as dense crowd and sparse crowd. Sparse crowd counting is relatively simple.\n",
        "\n",
        "<br>\n",
        "<hr>\n",
        "\n",
        "##**2. Methods used for Crowd Counting**\n",
        "\n",
        "###**2.1 Detection-based methods**\n",
        "\n",
        "> *Monolithic Detection:* Trains the classifier using the full-body appearance. Typical features include Haar wavelets, histogram of oriented gradient (HOG). ML algorithms include SVM, RF etc.\n",
        "\n",
        "> *Part-based detection:* Train the classifier using a part, say head or shoulders or head+shoulder and applies a classifier to it.\n",
        "\n",
        "> *Shape-based detection:* Draw boundaries around humans using ellipses, and then a stochastic process is used to estimate the number and shape.\n",
        "\n",
        "> **Cons:** Not good for dense crowds.\n",
        "\n",
        "\n",
        "\n",
        "###**2.2 Regression-based methods**\n",
        "\n",
        "> Features (e.g. edge details, foreground pixels) are extracted from the local image patches and mapped to the count.\n",
        "\n",
        "Example works include:\n",
        "[Paper](https://www.researchgate.net/publication257927731_Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation) \n",
        "[Paper](http://personal.ie.cuhk.edu.hk/~ccloy/files/bmvc_2012b.pdf)\n",
        "\n",
        "\n",
        "###**2.3 Density estimation-based methods**\n",
        "\n",
        "> Does not learn each individual separately, instead tracks a group of individuals at a time. Focuses on the density by learning the mapping between local features and object density maps.\n",
        "\n",
        "Example works include:\n",
        "[COUNT Forest](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Pham_COUNT_Forest_CO-Voting_ICCV_2015_paper.pdf)\n",
        "\n",
        "\n",
        "###**2.4 Deep Learning-based methods**\n",
        "\n",
        "> **Supervised DL Methods:** Use deep learning algorithms such as CNN and annotated images datasets.\n",
        "\n",
        "Example works include:\n",
        "[Paper](https://yangliang.github.io/pdf/sp055u.pdf) \n",
        "[Single Image Counting](https://pdfs.semanticscholar.org/7ca4/bcfb186958bafb1bb9512c40a9c54721c9fc.pdf)\n",
        "[CrowdNet](https://arxiv.org/pdf/1608.06197.pdf)\n",
        "\n",
        "> **Unsupervised DL Methods:** Use deep learning on un-annotated images datasets.\n",
        "\n",
        "Example works include:\n",
        "[Paper](https://pdfs.semanticscholar.org/d882/da8d831b7fb6c459f052f9e12c9a3ea9adac.pdf?_ga=2.13828146.448930578.1567343964-169603131.1567343964)\n",
        "\n",
        "<hr>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9sUQ1oGG06E",
        "colab_type": "text"
      },
      "source": [
        "##**3. Crowdcounting datasets**\n",
        "\n",
        "[**INRIA Person datasets**](http://pascal.inrialpes.fr/data/human/)\n",
        "> Dataset include a large set of marked up images of standing or walking people. It contains (a) original images with corresponding annotation files (a rectangular box around the detected object) with the corresponding class label) and (b) positive images in normalized 64x128 pixel format\n",
        "\n",
        "\n",
        "[**Mall Dataset**](http://personal.ie.cuhk.edu.hk/~ccloy/downloads_mall_dataset.html)\n",
        "> Composed by RGB images of frames in a video (as inputs) and the object counting on every frame, this is the number of pedestrians (object) in the image. This dataset can be used for regression. Over 60,000 pedestrians were labelled in 2000 video frames.\n",
        "\n",
        "\n",
        "[**Caltech Pedestrian Dataset**](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)\n",
        "> Consists of approximately 10 hours of 640x480 30Hz video taken from a vehicle driving through regular traffic in an urban environment. About 250,000 frames (in 137 approximately minute long segments) with a total of 350,000 bounding boxes and 2300 unique pedestrians were annotated. Annotation includes temporal correspondence between bounding boxes and detailed occlusion labels.\n",
        "\n",
        "[**ShanghaiTech Dataset**](https://svip-lab.github.io/datasets.html)\n",
        "> The dataset is available in two parts: Part A and Part B. Both datasets have unique data images. In each dataset , there are 3 folders: images, ground-truth and ground-truth-h5. [Paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhang_Single-Image_Crowd_Counting_CVPR_2016_paper.pdf)\n",
        "\n",
        "\n",
        "[**World Expo Dataset**](http://www.ee.cuhk.edu.hk/~xgwang/datasets.html)\n",
        "> Dataset is splitted into two parts:  1,127 one-minute long video sequences out of 103 scenes are treated as training and validation sets. 3 labeled frames in each training video and the interval between two labeled frames is 15 seconds.\n",
        "\n",
        "[**Grand Central Station Dataset**](http://www.ee.cuhk.edu.hk/~xgwang/grandcentral.html)\n",
        "> Video of length: 33:20 minutes, 50010 frames and Frame Rate: 25fps, 720x480. Dataset include 3 files: Video, Trajectories and TrajectoriesNew.\n",
        "\n",
        "[**UCF CC 50**](https://www.crcv.ucf.edu/data/ucf-cc-50/)\n",
        "> Contains images of extremely dense crowds, collected mainly from the FLICKR. [Paper](https://www.crcv.ucf.edu/papers/cvpr2013/Counting_V3o.pdf)\n",
        "\n",
        "[**UCF-QNRF**](https://www.crcv.ucf.edu/data/ucf-qnrf/)\n",
        "> Contains 1535 images which are divided into train and test sets of 1201 and 334 images respectively. [Paper](https://www.crcv.ucf.edu/papers/eccv2018/2324.pdf)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "**Extra: Crowdcounting Frameworks**\n",
        "\n",
        "> [C³ Framework:](https://arxiv.org/pdf/1907.02724)  works with 6 main datasets UCF CC 50, WorldExpo’10, SHT A, SHT B, UCF-QNRF, and GCC. It is based on Pytorch code.\n",
        "\n",
        "> [CSRNet](https://arxiv.org/pdf/1802.10062v4.pdf) Works with ShanghaiTech, UCF_CC_50, WorldEXPO’10, and UCSD dataset.\n",
        "\n",
        "> [PCC Net](https://github.com/gjy3035/PCC-Net) CNN based crowd counting technique by generating density."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfqucFAYQ_Oq",
        "colab_type": "text"
      },
      "source": [
        "##**Sparse Crowd Counting in a Single Image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mADkL7kVQwdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}